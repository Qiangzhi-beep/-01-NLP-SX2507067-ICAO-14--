"""
æ™ºèƒ½é—®ç­”ç³»ç»Ÿ v1.0
ä½œè€…ï¼šAIåŠ©æ‰‹
åŠŸèƒ½ï¼šåŸºäºæœ¬åœ°çŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
"""

import os
import json
import requests
import sys
from typing import List, Dict, Tuple, Optional
import numpy as np
from datetime import datetime
import hashlib
import time

# å®‰è£…å¿…è¦ä¾èµ–ï¼š
# pip install chromadb langchain sentence-transformers requests beautifulsoup4 markdown tiktoken

try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
    import markdown
    from bs4 import BeautifulSoup
    import re
    import tiktoken
except ImportError as e:
    print(f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“: {e}")
    print("è¯·è¿è¡Œ: pip install chromadb sentence-transformers requests beautifulsoup4 markdown tiktoken")
    sys.exit(1)


class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ç®¡ç†"""

    def __init__(self, knowledge_base_path: str):
        """
        åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“

        Args:
            knowledge_base_path: .mdæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        """
        self.knowledge_base_path = knowledge_base_path
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        # åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“
        chroma_persist_dir = "D:\\chroma_db"  # æŒä¹…åŒ–å­˜å‚¨ç›®å½•

        try:
            self.chroma_client = chromadb.PersistentClient(
                path=chroma_persist_dir,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )

            # åˆ›å»ºæˆ–è·å–é›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="airport_knowledge_base",
                metadata={"hnsw:space": "cosine"}
            )

            print(f"âœ… Chromaæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸï¼Œå­˜å‚¨è·¯å¾„: {chroma_persist_dir}")

        except Exception as e:
            print(f"âŒ Chromaæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°è¯•åˆ›å»ºå†…å­˜æ•°æ®åº“...")
            # å›é€€åˆ°å†…å­˜æ•°æ®åº“
            self.chroma_client = chromadb.EphemeralClient()
            self.collection = self.chroma_client.create_collection(
                name="airport_knowledge_base",
                metadata={"hnsw:space": "cosine"}
            )

        # åŠ è½½çŸ¥è¯†åº“
        self.load_knowledge_base()

    def read_markdown_file(self, filepath: str) -> str:
        """è¯»å–Markdownæ–‡ä»¶å†…å®¹"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            return content
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {str(e)}")
            return ""

    def split_document(self, content: str, filename: str, max_chunk_size: int = 800) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æ¡£ä¸ºç‰‡æ®µ"""
        chunks = []

        # 1. é¦–å…ˆå°è¯•æŒ‰æ ‡é¢˜åˆ†å‰²
        # åŒ¹é… #, ##, ### ç­‰æ ‡é¢˜
        heading_pattern = r'(?m)^(#{1,3})\s+(.+?)$'
        sections = re.split(heading_pattern, content)

        current_chunk = ""
        current_section = ""

        if len(sections) > 1:
            # æœ‰æ ‡é¢˜çš„æƒ…å†µ
            for i in range(1, len(sections), 3):
                if i + 2 < len(sections):
                    heading_level = sections[i]
                    heading_text = sections[i + 1]
                    section_content = sections[i + 2] if i + 2 < len(sections) else ""

                    # åˆ›å»ºåŒ…å«æ ‡é¢˜çš„å—
                    chunk = f"{heading_level} {heading_text}\n{section_content}"

                    # å¦‚æœå—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                    if len(chunk) > max_chunk_size:
                        sub_chunks = self.split_by_paragraphs(chunk, max_chunk_size)
                        chunks.extend(sub_chunks)
                    else:
                        chunks.append(chunk.strip())
        else:
            # æ²¡æœ‰æ ‡é¢˜ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            chunks = self.split_by_paragraphs(content, max_chunk_size)

        return chunks

    def split_by_paragraphs(self, text: str, max_chunk_size: int) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            if len(current_chunk) + len(para) + 2 <= max_chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“åˆ°å‘é‡æ•°æ®åº“"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")

        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ•°æ®
        try:
            count = self.collection.count()
            if count > 0:
                print(f"ğŸ“š çŸ¥è¯†åº“å·²å­˜åœ¨ï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                return
        except:
            pass

        documents = []
        metadatas = []
        ids = []

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.knowledge_base_path):
            print(f"âŒ è­¦å‘Šï¼šçŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {self.knowledge_base_path}")
            print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return

        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶
        if os.path.isfile(self.knowledge_base_path) and self.knowledge_base_path.endswith('.md'):
            print(f"ğŸ“„ åŠ è½½å•ä¸ªæ–‡ä»¶: {self.knowledge_base_path}")
            files = [self.knowledge_base_path]
        # å¦‚æœæ˜¯ç›®å½•
        elif os.path.isdir(self.knowledge_base_path):
            print(f"ğŸ“ åŠ è½½ç›®å½•: {self.knowledge_base_path}")
            files = [os.path.join(self.knowledge_base_path, f) for f in os.listdir(self.knowledge_base_path)
                     if f.endswith('.md')]
        else:
            print(f"âŒ é”™è¯¯ï¼šè·¯å¾„æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {self.knowledge_base_path}")
            return

        if not files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•.mdæ–‡ä»¶")
            return

        total_chunks = 0

        for filepath in files:
            try:
                filename = os.path.basename(filepath)
                print(f"  æ­£åœ¨å¤„ç†: {filename}")

                content = self.read_markdown_file(filepath)
                if not content:
                    continue

                # åˆ†å‰²æ–‡æ¡£
                chunks = self.split_document(content, filename)

                for i, chunk in enumerate(chunks):
                    chunk_id = f"{filename}_{i}_{hashlib.md5(chunk.encode()).hexdigest()[:8]}"
                    documents.append(chunk)
                    metadatas.append({
                        "source": filename,
                        "filepath": filepath,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "timestamp": datetime.now().isoformat()
                    })
                    ids.append(chunk_id)
                    total_chunks += 1

                    if total_chunks % 50 == 0:
                        print(f"  å·²å¤„ç† {total_chunks} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")

            except Exception as e:
                print(f"  å¤„ç†æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {str(e)}")
                continue

        if documents:
            print(f"ğŸ“Š æ­£åœ¨ä¸º {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡...")

            try:
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜ä¸è¶³
                batch_size = 100
                all_embeddings = []

                for i in range(0, len(documents), batch_size):
                    batch_docs = documents[i:i + batch_size]
                    print(f"  å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(documents) - 1) // batch_size + 1}")

                    batch_embeddings = self.embedding_model.encode(batch_docs).tolist()
                    all_embeddings.extend(batch_embeddings)

                print("âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨æ·»åŠ åˆ°æ•°æ®åº“...")

                # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                self.collection.add(
                    embeddings=all_embeddings,
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )

                print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

            except Exception as e:
                print(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„æ–‡æ¡£å†…å®¹")

    def search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3) -> List[Dict]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³é—®é¢˜

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query]).tolist()

            # æœç´¢å‘é‡æ•°æ®åº“
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )

            # å¤„ç†ç»“æœ
            relevant_docs = []
            if results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                        results['documents'][0],
                        results['metadatas'][0],
                        results['distances'][0]
                )):
                    # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                    similarity = 1 - distance

                    if similarity >= similarity_threshold:
                        relevant_docs.append({
                            "content": doc,
                            "metadata": metadata,
                            "similarity": similarity,
                            "source": "local_knowledge_base"
                        })

            return relevant_docs

        except Exception as e:
            print(f"âŒ æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")
            return []


class WebSearch:
    """ç½‘ç»œæœç´¢æ¨¡å—"""

    def __init__(self, silicon_flow_api_key: str):
        """
        åˆå§‹åŒ–ç½‘ç»œæœç´¢æ¨¡å—

        Args:
            silicon_flow_api_key: ç¡…åŸºæµåŠ¨APIå¯†é’¥
        """
        self.api_key = silicon_flow_api_key
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"

    def search_web(self, query: str) -> Optional[Dict]:
        """
        ä½¿ç”¨ç¡…åŸºæµåŠ¨APIè¿›è¡Œç½‘ç»œæœç´¢
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        # æ„é€ è¯·æ±‚æ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚
                å¦‚æœé—®é¢˜æ¶‰åŠä¸“ä¸šé¢†åŸŸï¼Œè¯·ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚
                å¦‚æœæ— æ³•æ‰¾åˆ°ç¡®åˆ‡ç­”æ¡ˆï¼Œè¯·æä¾›ç›¸å…³ä¿¡æ¯å’Œè¿›ä¸€æ­¥æŸ¥è¯¢çš„å»ºè®®ã€‚"""
            },
            {
                "role": "user",
                "content": f"è¯·æä¾›å…³äºä»¥ä¸‹é—®é¢˜çš„è¯¦ç»†ã€å‡†ç¡®çš„ä¿¡æ¯ï¼š{query}\nè¯·ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚"
            }
        ]

        try:
            payload = {
                "model": "Qwen/Qwen2.5-72B-Instruct",  # ä½¿ç”¨Qwenæ¨¡å‹
                "messages": messages,
                "temperature": 0.3,
                "max_tokens": 1500,
                "stream": False
            }

            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)

            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']

                return {
                    "content": content,
                    "source": "web_search",
                    "confidence": 0.8,
                    "model": "Qwen2.5-72B"
                }
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text}")
                return None

        except requests.exceptions.Timeout:
            print("âŒ ç½‘ç»œæœç´¢è¯·æ±‚è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ ç½‘ç»œæœç´¢æ—¶å‡ºé”™: {str(e)}")
            return None


class SmartQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, knowledge_base_path: str, silicon_flow_api_key: str):
        """
        åˆå§‹åŒ–æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

        Args:
            knowledge_base_path: çŸ¥è¯†åº“è·¯å¾„
            silicon_flow_api_key: ç¡…åŸºæµåŠ¨APIå¯†é’¥
        """
        print("=" * 60)
        print("ğŸš€ æ™ºèƒ½é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–ä¸­...")
        print("=" * 60)

        # åˆå§‹åŒ–çŸ¥è¯†åº“
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")
        self.knowledge_base = LocalKnowledgeBase(knowledge_base_path)

        # åˆå§‹åŒ–ç½‘ç»œæœç´¢
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç½‘ç»œæœç´¢æ¨¡å—...")
        self.web_search = WebSearch(silicon_flow_api_key)

        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        self.conversation_history = []
        self.fallback_count = 0
        self.max_fallback_before_escalation = 3
        self.user_name = "ç”¨æˆ·"

        print("=" * 60)
        print("âœ… æ™ºèƒ½é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ“ çŸ¥è¯†åº“è·¯å¾„: {knowledge_base_path}")
        print(f"ğŸ”‘ APIå¯†é’¥: {silicon_flow_api_key[:12]}...{silicon_flow_api_key[-8:]}")
        print("=" * 60)
        print("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥'é€€å‡º'æˆ–'quit'ç»“æŸï¼‰\n")

    def classify_question(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜ç±»å‹"""
        question_lower = question.lower()

        # æœºåœº/ç›´å‡æœºç›¸å…³å…³é”®è¯
        airport_keywords = ['æœºåœº', 'è·‘é“', 'èˆªç«™æ¥¼', 'å¡”å°', 'åœæœºåª', 'å®‰æ£€', 'æµ·å…³']
        helicopter_keywords = ['ç›´å‡æœº', 'æ—‹ç¿¼', 'èµ·é™', 'åœæœºåª', 'æ—‹ç¿¼æœº']
        aviation_keywords = ['èˆªç©º', 'é£è¡Œ', 'é£è¡Œå‘˜', 'ç©ºç®¡', 'å¯¼èˆª', 'ä»ªè¡¨']

        # é—®é¢˜ç±»å‹æ£€æµ‹
        if any(kw in question_lower for kw in ['æ€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·', 'æ­¥éª¤', 'æ–¹æ³•', 'æ“ä½œ']):
            return "how_to"
        elif any(kw in question_lower for kw in ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'è§£é‡Š']):
            return "definition"
        elif any(kw in question_lower for kw in ['ä¸ºä»€ä¹ˆ', 'åŸå› ', 'ä¸ºä½•', 'åŸç†']):
            return "explanation"
        elif any(kw in question_lower for kw in ['åŒºåˆ«', 'æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'ä¸åŒ']):
            return "comparison"
        elif any(kw in question_lower for kw in ['è°', 'ä½•æ—¶', 'å“ªé‡Œ', 'å¤šå°‘', 'å“ªäº›', 'æ˜¯å¦']):
            return "factual"
        elif any(kw in question_lower for kw in airport_keywords + helicopter_keywords + aviation_keywords):
            return "aviation_specific"
        else:
            return "general"

    def extract_keywords(self, question: str) -> List[str]:
        """ä»é—®é¢˜ä¸­æå–å…³é”®è¯"""
        # å»é™¤åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'åœ¨', 'æœ‰', 'æ›´', 'è¿™ä¸ª', 'ä¸€ä¸ª',
                      'å—', 'å‘¢', 'å•Š', 'å‘€'}

        # æå–ä¸­æ–‡å’Œè‹±æ–‡å•è¯
        words = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', question)
        keywords = [word for word in words if word not in stop_words and len(word) > 1]

        # ä¿ç•™å‰5ä¸ªå…³é”®è¯
        return keywords[:5]

    def get_available_topics(self) -> List[str]:
        """è·å–çŸ¥è¯†åº“ä¸­å¯ç”¨çš„ä¸»é¢˜"""
        try:
            # ä»å‘é‡æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
            all_docs = self.knowledge_base.collection.get()

            topics_set = set()
            if all_docs and 'metadatas' in all_docs:
                for metadata in all_docs['metadatas']:
                    if isinstance(metadata, dict):
                        source = metadata.get('source', '')
                        if source and source.endswith('.md'):
                            # å»é™¤.mdåç¼€ï¼Œè·å–ä¸»é¢˜å
                            topic = os.path.splitext(source)[0]
                            topics_set.add(topic)

            topics = list(topics_set)
            return sorted(topics)[:10]  # è¿”å›å‰10ä¸ªæ’åºåçš„ä¸»é¢˜

        except Exception as e:
            print(f"è·å–ä¸»é¢˜åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")

        # é»˜è®¤ä¸»é¢˜ï¼ˆå¦‚æœè·å–å¤±è´¥ï¼‰
        return ["æœºåœºè®¾è®¡", "ç›´å‡æœºæœºåœº", "é£è¡Œè§„åˆ™", "å®‰å…¨æ ‡å‡†", "è¿è¡Œç®¡ç†"]

    def format_answer(self, doc: Dict, is_multiple: bool = False) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ–‡æ¡£çš„å›ç­”"""
        content = doc['content']
        similarity = doc['similarity']
        source = doc['metadata'].get('source', 'æœªçŸ¥æ–‡æ¡£')

        # é™åˆ¶å†…å®¹é•¿åº¦
        if len(content) > 800:
            content = content[:800] + "...\n\nï¼ˆå†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´ä¿¡æ¯è¯·æŸ¥çœ‹åŸæ–‡æ¡£ï¼‰"

        response = ""
        if not is_multiple:
            response += f"ğŸ“š **æ¥è‡ªã€Š{source}ã€‹** (ç›¸å…³åº¦: {similarity:.1%})\n\n"

        response += f"{content}\n"

        if not is_multiple:
            response += f"\n---\n*æ¥æº: {source}*"

        return response

    def format_partial_answer(self, relevant_docs: List[Dict], question: str) -> str:
        """æ ¼å¼åŒ–éƒ¨åˆ†ç›¸å…³çš„ç­”æ¡ˆ"""
        if not relevant_docs:
            return self.generate_smart_options(question)

        response = "ğŸ¤” æˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸å…³ä¿¡æ¯ï¼Œä½†å¯èƒ½ä¸å®Œå…¨åŒ¹é…æ‚¨çš„é—®é¢˜ï¼š\n\n"

        for i, doc in enumerate(relevant_docs[:3], 1):
            content_preview = doc['content'][:300] + "..." if len(doc['content']) > 300 else doc['content']
            source = doc['metadata'].get('source', 'æœªçŸ¥æ–‡æ¡£')
            similarity = doc['similarity']

            response += f"**é€‰é¡¹ {i}** - æ¥è‡ªã€Š{source}ã€‹ (ç›¸å…³åº¦: {similarity:.1%})\n"
            response += f"{content_preview}\n\n"

        response += "è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰å¸®åŠ©å—ï¼Ÿæˆ–è€…æ‚¨å¯ä»¥ï¼š\n"
        response += "1ï¸âƒ£ é€‰æ‹©å…¶ä¸­ä¸€ä¸ªé€‰é¡¹æŸ¥çœ‹è¯¦æƒ…\n"
        response += "2ï¸âƒ£ **å¯ç”¨ç½‘ç»œæœç´¢**è·å–æ›´å¹¿æ³›ä¿¡æ¯\n"
        response += "3ï¸âƒ£ **é‡æ–°è¡¨è¿°**æ‚¨çš„é—®é¢˜\n"
        response += "4ï¸âƒ£ **æŸ¥çœ‹å…¶ä»–ç›¸å…³ä¸»é¢˜**\n\n"
        response += "è¯·å›å¤æ•°å­—é€‰æ‹©ç›¸åº”æ“ä½œã€‚"

        return response

    def generate_smart_options(self, question: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½é€‰é¡¹èœå•"""
        keywords = self.extract_keywords(question)
        available_topics = self.get_available_topics()
        question_type = self.classify_question(question)

        response = "ğŸ” æˆ‘æœªèƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç¡®åˆ‡ç­”æ¡ˆã€‚æ‚¨å¯ä»¥é€‰æ‹©ä»¥ä¸‹æ“ä½œï¼š\n\n"

        # åŸºç¡€é€‰é¡¹
        options = [
            "1ï¸âƒ£ ğŸ“ **é‡æ–°è¡¨è¿°é—®é¢˜**ï¼ˆå½“å‰è¡¨è¿°å¯èƒ½ä¸å¤Ÿæ˜ç¡®ï¼‰",
            "2ï¸âƒ£ ğŸ” **é™ä½æœç´¢æ ‡å‡†**ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶ï¼‰",
            "3ï¸âƒ£ ğŸŒ **å¯ç”¨ç½‘ç»œæœç´¢**ï¼ˆè·å–æœ€æ–°ã€æ›´å¹¿æ³›çš„ä¿¡æ¯ï¼‰",
            f"4ï¸âƒ£ ğŸ“‚ **æµè§ˆçŸ¥è¯†åº“ä¸»é¢˜**ï¼ˆå½“å‰åŒ…å«ï¼š{', '.join(available_topics[:3])}ç­‰ï¼‰",
        ]

        # æ ¹æ®å…³é”®è¯æ·»åŠ é€‰é¡¹
        if keywords:
            options.append(f"5ï¸âƒ£ ğŸ”‘ **ä½¿ç”¨å…³é”®è¯æœç´¢**ï¼š{', '.join(keywords)}")

        # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ ç‰¹å®šé€‰é¡¹
        if question_type == "aviation_specific":
            options.append("6ï¸âƒ£ âœˆï¸ **æŸ¥çœ‹èˆªç©ºä¸“ä¸šçŸ¥è¯†åº“**")
        elif question_type == "how_to":
            options.append("6ï¸âƒ£ ğŸ“‹ **æŸ¥çœ‹æ“ä½œæŒ‡å—ç±»æ–‡æ¡£**")
        elif question_type == "definition":
            options.append("6ï¸âƒ£ ğŸ“š **æŸ¥çœ‹æœ¯è¯­å®šä¹‰ç±»æ–‡æ¡£**")

        options.append("0ï¸âƒ£ â“ **è·å–ç³»ç»Ÿå¸®åŠ©**ï¼ˆæŸ¥çœ‹ä½¿ç”¨æŒ‡å—ï¼‰")

        response += "\n".join(options)
        response += "\n\nğŸ’¡ æç¤ºï¼šç›´æ¥å›å¤æ•°å­—å³å¯é€‰æ‹©ç›¸åº”æ“ä½œã€‚"

        return response

    def process_user_choice(self, choice: str, question: str) -> str:
        """å¤„ç†ç”¨æˆ·é€‰æ‹©çš„é€‰é¡¹"""
        choice = choice.strip()

        if choice == "1" or choice == "1ï¸âƒ£":
            return "ğŸ’¬ è¯·ç”¨æ›´å…·ä½“ã€æ›´æ˜ç¡®çš„è¡¨è¿°é‡æ–°æé—®ï¼Œä¾‹å¦‚ï¼š\nâ€¢ 'ç›´å‡æœºæœºåœºçš„è®¾è®¡æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ'\nâ€¢ 'æœºåœºè·‘é“é•¿åº¦æœ‰å“ªäº›è¦æ±‚ï¼Ÿ'"

        elif choice == "2" or choice == "2ï¸âƒ£":
            # é™ä½é˜ˆå€¼é‡æ–°æœç´¢
            relevant_docs = self.knowledge_base.search(question, similarity_threshold=0.1)
            if relevant_docs:
                return self.format_partial_answer(relevant_docs, question)
            else:
                return "âš ï¸ å³ä½¿é™ä½æœç´¢æ ‡å‡†ï¼ŒçŸ¥è¯†åº“ä¸­ä»æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®å¯ç”¨ç½‘ç»œæœç´¢ã€‚"

        elif choice == "3" or choice == "3ï¸âƒ£":
            # å¯ç”¨ç½‘ç»œæœç´¢
            print("ğŸŒ æ­£åœ¨æœç´¢ç½‘ç»œä¿¡æ¯ï¼Œè¯·ç¨å€™...")
            web_result = self.web_search.search_web(question)
            if web_result:
                return f"ğŸŒ **ç½‘ç»œæœç´¢ç»“æœ** (æ¨¡å‹: {web_result.get('model', 'æœªçŸ¥')}):\n\n{web_result['content']}\n\n---\n*æ¥æºï¼šç¡…åŸºæµåŠ¨ç½‘ç»œæœç´¢*"
            else:
                return "âŒ ç½‘ç»œæœç´¢å¤±è´¥ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"

        elif choice == "4" or choice == "4ï¸âƒ£":
            topics = self.get_available_topics()
            return f"ğŸ“š **çŸ¥è¯†åº“åŒ…å«ä»¥ä¸‹ä¸»é¢˜**ï¼š\n\n" + "\n".join(
                [f"â€¢ {topic}" for topic in topics]) + "\n\nğŸ’¡ æ‚¨å¯ä»¥é’ˆå¯¹è¿™äº›ä¸»é¢˜æé—®ã€‚"

        elif choice == "5" or choice == "5ï¸âƒ£":
            keywords = self.extract_keywords(question)
            return f"ğŸ”‘ **å»ºè®®æœç´¢å…³é”®è¯**ï¼š\n\n" + "\n".join([f"â€¢ `{kw}`" for kw in keywords]) + "\n\nğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å…³é”®è¯é‡æ–°æœç´¢ã€‚"

        elif choice == "6" or choice == "6ï¸âƒ£":
            q_type = self.classify_question(question)
            if q_type == "aviation_specific":
                return "âœˆï¸ **èˆªç©ºä¸“ä¸šçŸ¥è¯†åº“**ï¼š\n\n1. æœºåœºè®¾è®¡æ ‡å‡†\n2. ç›´å‡æœºè¿è¡Œè§„èŒƒ\n3. é£è¡Œå®‰å…¨è§„åˆ™\n4. ç©ºç®¡é€šä¿¡æµç¨‹\n5. åº”æ€¥å¤„ç†ç¨‹åº\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£çš„å…·ä½“å†…å®¹ã€‚"
            elif q_type == "how_to":
                return "ğŸ“‹ **æ“ä½œæŒ‡å—ç±»æ–‡æ¡£**ï¼š\n\n1. æœºåœºå»ºè®¾æ­¥éª¤\n2. ç›´å‡æœºèµ·é™æ“ä½œ\n3. å®‰å…¨æ£€æŸ¥æµç¨‹\n4. è®¾å¤‡ç»´æŠ¤æ–¹æ³•\n5. åº”æ€¥å¤„ç½®ç¨‹åº"
            elif q_type == "definition":
                return "ğŸ“š **æœ¯è¯­å®šä¹‰ç±»æ–‡æ¡£**ï¼š\n\n1. èˆªç©ºæœ¯è¯­è¡¨\n2. æŠ€æœ¯å‚æ•°å®šä¹‰\n3. æ³•è§„æ ‡å‡†è§£é‡Š\n4. ä¸“ä¸šåè¯é‡Šä¹‰"
            else:
                return "6ï¸âƒ£ é€‰é¡¹å·²é€‰æ‹©ï¼Œè¯·å…·ä½“è¯´æ˜æ‚¨éœ€è¦å“ªæ–¹é¢çš„å¸®åŠ©ã€‚"

        elif choice == "0" or choice == "0ï¸âƒ£":
            return self.get_help_info()

        else:
            return "âŒ æ— æ•ˆçš„é€‰æ‹©ã€‚è¯·å›å¤æ•°å­—1-6æˆ–0é€‰æ‹©ç›¸åº”æ“ä½œã€‚"

    def get_help_info(self) -> str:
        """è·å–ç³»ç»Ÿå¸®åŠ©ä¿¡æ¯"""
        doc_count = self.knowledge_base.collection.count()

        help_text = f"""
ğŸ¤– **æ™ºèƒ½é—®ç­”ç³»ç»Ÿ v1.0 - å¸®åŠ©æŒ‡å—**

**ç³»ç»ŸçŠ¶æ€**
â€¢ ğŸ“Š çŸ¥è¯†åº“æ–‡æ¡£æ•°: {doc_count} ä¸ªç‰‡æ®µ
â€¢ ğŸŒ ç½‘ç»œæœç´¢: {'âœ… å·²å¯ç”¨' if hasattr(self, 'web_search') else 'âŒ æœªå¯ç”¨'}
â€¢ ğŸ’¾ å¯¹è¯å†å²: {len(self.conversation_history) // 2} è½®å¯¹è¯

**ä¸»è¦åŠŸèƒ½**
1. **æœ¬åœ°çŸ¥è¯†åº“é—®ç­”** - åŸºäºæ‚¨æä¾›çš„æœºåœº/ç›´å‡æœºæ–‡æ¡£
2. **æ™ºèƒ½ç½‘ç»œæœç´¢** - ä½¿ç”¨ç¡…åŸºæµåŠ¨APIè·å–æœ€æ–°ä¿¡æ¯
3. **å¤šçº§å“åº”ç³»ç»Ÿ** - æ ¹æ®åŒ¹é…ç¨‹åº¦æä¾›ä¸åŒçº§åˆ«çš„å›ç­”
4. **ä¼šè¯è®°å¿†** - ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡

**ä½¿ç”¨æŠ€å·§**
â€¢ æé—®å°½é‡**å…·ä½“æ˜ç¡®**ï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°
â€¢ ä½¿ç”¨**å®Œæ•´çš„é—®é¢˜å¥å¼**ï¼Œå¦‚"ä»€ä¹ˆæ˜¯ç›´å‡æœºæœºåœºçš„è®¾è®¡æ ‡å‡†ï¼Ÿ"
â€¢ å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥**åˆ†æ­¥éª¤æé—®**
â€¢ å–„ç”¨ç³»ç»Ÿæä¾›çš„**é€‰é¡¹èœå•**å¼•å¯¼æœç´¢
â€¢ è¾“å…¥"é€€å‡º"æˆ–"quit"ç»“æŸå¯¹è¯

**æ”¯æŒçš„é—®é¢˜ç±»å‹**
â€¢ âœˆï¸ èˆªç©ºä¸“ä¸šé—®é¢˜ï¼ˆæœºåœºã€ç›´å‡æœºç­‰ï¼‰
â€¢ ğŸ“‹ æ“ä½œæŒ‡å—ç±»é—®é¢˜
â€¢ ğŸ“š å®šä¹‰è§£é‡Šç±»é—®é¢˜
â€¢ ğŸ” äº‹å®æŸ¥è¯¢ç±»é—®é¢˜
â€¢ ğŸ”„ æ¯”è¾ƒåˆ†æç±»é—®é¢˜

**å¸¸ç”¨å‘½ä»¤**
â€¢ å¸®åŠ© - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
â€¢ çŠ¶æ€ - æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
â€¢ ä¸»é¢˜ - æŸ¥çœ‹çŸ¥è¯†åº“ä¸»é¢˜
â€¢ å†å² - æŸ¥çœ‹å¯¹è¯å†å²
â€¢ æ¸…é™¤ - æ¸…é™¤å½“å‰å¯¹è¯å†å²

**é—®é¢˜åé¦ˆ**
å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è®°å½•ä¸‹æ‚¨çš„æé—®å’Œç³»ç»Ÿçš„å“åº”ã€‚
        """
        return help_text

    def ask(self, question: str) -> str:
        """
        ä¸»é—®ç­”æ¥å£

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            å›ç­”å†…å®¹
        """
        # æ¸…ç†è¾“å…¥
        question = question.strip()

        if not question:
            return "è¯·æå‡ºæ‚¨çš„é—®é¢˜ã€‚"

        # æ£€æŸ¥ç‰¹æ®Šå‘½ä»¤
        if question.lower() in ['å¸®åŠ©', 'help', '?']:
            return self.get_help_info()
        elif question.lower() in ['çŠ¶æ€', 'status']:
            return f"ğŸ“Š ç³»ç»ŸçŠ¶æ€ï¼šçŸ¥è¯†åº“ç‰‡æ®µæ•°={self.knowledge_base.collection.count()}, å¯¹è¯è½®æ•°={len(self.conversation_history) // 2}"
        elif question.lower() in ['ä¸»é¢˜', 'topics', 'ç›®å½•']:
            topics = self.get_available_topics()
            return f"ğŸ“š çŸ¥è¯†åº“ä¸»é¢˜ï¼š\n" + "\n".join([f"â€¢ {t}" for t in topics])
        elif question.lower() in ['å†å²', 'history']:
            if len(self.conversation_history) > 0:
                history_text = "ğŸ“ å¯¹è¯å†å²ï¼š\n"
                for i, entry in enumerate(self.conversation_history[-10:]):  # æ˜¾ç¤ºæœ€å10æ¡
                    role = "ğŸ‘¤" if entry.get('role') == 'user' else "ğŸ¤–"
                    history_text += f"{i + 1}. {role}: {entry.get('content', '')[:50]}...\n"
                return history_text
            else:
                return "ğŸ“ å½“å‰æ²¡æœ‰å¯¹è¯å†å²ã€‚"
        elif question.lower() in ['æ¸…é™¤', 'clear', 'é‡ç½®']:
            self.conversation_history = []
            self.fallback_count = 0
            return "âœ… å¯¹è¯å†å²å·²æ¸…é™¤ã€‚"

        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            "role": "user",
            "content": question,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })

        # æ£€æŸ¥æ˜¯å¦ä¸ºé€‰é¡¹é€‰æ‹©
        if len(self.conversation_history) >= 2:
            last_response = self.conversation_history[-2].get("content", "")
            if "è¯·å›å¤æ•°å­—" in last_response or "è¯·å›å¤" in last_response:
                # ç”¨æˆ·åœ¨é€‰æ‹©èœå•é€‰é¡¹
                return self.process_user_choice(question, "")

        print(f"ğŸ” æ­£åœ¨æœç´¢æœ¬åœ°çŸ¥è¯†åº“...")

        # 1. æœç´¢æœ¬åœ°çŸ¥è¯†åº“
        relevant_docs = self.knowledge_base.search(question)

        # 2. åˆ†æç»“æœ
        if relevant_docs:
            # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
            max_similarity = max([doc.get('similarity', 0) for doc in relevant_docs])

            if max_similarity > 0.7:
                # é«˜ç½®ä¿¡åº¦ç»“æœ
                self.fallback_count = 0
                best_doc = max(relevant_docs, key=lambda x: x.get('similarity', 0))
                response = self.format_answer(best_doc)

            elif max_similarity > 0.4:
                # ä¸­ç­‰ç½®ä¿¡åº¦ç»“æœ
                self.fallback_count = min(self.fallback_count + 1, self.max_fallback_before_escalation)
                response = self.format_partial_answer(relevant_docs, question)

            else:
                # ä½ç½®ä¿¡åº¦ç»“æœ
                self.fallback_count += 1
                if self.fallback_count >= self.max_fallback_before_escalation:
                    response = f"âš ï¸ å¤šæ¬¡æœç´¢æœªæ‰¾åˆ°æ»¡æ„ç­”æ¡ˆã€‚\n\n" + self.generate_smart_options(question)
                else:
                    response = self.generate_smart_options(question)

        else:
            # æ²¡æœ‰æ‰¾åˆ°ç»“æœ
            self.fallback_count += 1

            if self.fallback_count >= self.max_fallback_before_escalation:
                response = f"âŒ è¿ç»­{self.fallback_count}æ¬¡æœªæ‰¾åˆ°ç­”æ¡ˆã€‚\n\n" + self.generate_smart_options(question)
            else:
                response = self.generate_smart_options(question)

        # è®°å½•ç³»ç»Ÿå“åº”
        self.conversation_history.append({
            "role": "assistant",
            "content": response,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })

        return response


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("\n" + "=" * 60)
    print("ğŸš€ æœºåœº-ç›´å‡æœºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ v1.0")
    print("=" * 60)

    # é…ç½®ä¿¡æ¯
    KNOWLEDGE_BASE_PATH = r"D:\AlgorithmClub\Damoxingyuanli\homework\datas\é™„ä»¶14 æœºåœº â€” ç›´å‡æœºåœº _Volume II\index.md"
    SILICON_FLOW_API_KEY = "sk-bdgrimfksplnwstzulxfsrdijhjqribunforxvknatzpjlui"

    # éªŒè¯è·¯å¾„
    if not os.path.exists(KNOWLEDGE_BASE_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return

    # åˆ›å»ºé—®ç­”ç³»ç»Ÿå®ä¾‹
    try:
        qa_system = SmartQASystem(KNOWLEDGE_BASE_PATH, SILICON_FLOW_API_KEY)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # äº¤äº’å¾ªç¯
    conversation_count = 0

    while True:
        try:
            user_input = input(f"\nğŸ‘¤ ç¬¬{conversation_count + 1}è½®æé—®: ").strip()
            conversation_count += 1

            if user_input.lower() in ['é€€å‡º', 'quit', 'exit', 'bye', 'å†è§']:
                print("\nğŸ¤– è°¢è°¢ä½¿ç”¨ï¼å†è§ï¼")
                break

            if not user_input:
                continue

            # è·å–å›ç­”
            response = qa_system.ask(user_input)
            print(f"\nğŸ¤– å›ç­”: {response}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å¯¹è¯å·²ä¸­æ–­ã€‚")
            break
        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
            print("è¯·é‡æ–°æé—®æˆ–æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")


if __name__ == "__main__":
    main()